raise RuntimeError("âœ… å¦‚æœä½ çœ‹åˆ°è¿™ä¸ªï¼Œè¯´æ˜è„šæœ¬ç¡®å®æ‰§è¡Œäº†")

import os
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader
from tqdm import tqdm

# é…ç½®
DATA_DIR = './data'
MODEL_PATH = 'emotion_model.pt'
NUM_CLASSES = 7
BATCH_SIZE = 64
EPOCHS = 10
LR = 0.001

# å›¾åƒé¢„å¤„ç†
transform = transforms.Compose([
    transforms.Grayscale(),
    transforms.Resize((48, 48)),
    transforms.ToTensor()
])

# æ¨¡å‹ç»“æ„
class EmotionCNN(nn.Module):
    def __init__(self):
        super().__init__()
        self.net = nn.Sequential(
            nn.Conv2d(1, 32, 3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2, 2),
            nn.Conv2d(32, 64, 3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2, 2),
            nn.Flatten(),
            nn.Linear(64 * 12 * 12, 128),
            nn.ReLU(),
            nn.Linear(128, NUM_CLASSES)
        )

    def forward(self, x):
        return self.net(x)

# ä¸»ç¨‹åº
if __name__ == "__main__":
    print("ğŸš€ æ­£åœ¨è¿è¡Œä¸»å‡½æ•°", flush=True)

    # åŠ è½½æ•°æ®
    train_dataset = datasets.ImageFolder(os.path.join(DATA_DIR, 'train'), transform=transform)
    test_dataset = datasets.ImageFolder(os.path.join(DATA_DIR, 'test'), transform=transform)
    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)
    print("ğŸ“¦ å·²ç»åŠ è½½æ•°æ®ï¼", flush=True)

    # åˆå§‹åŒ–æ¨¡å‹
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = EmotionCNN().to(device)
    optimizer = optim.Adam(model.parameters(), lr=LR)
    criterion = nn.CrossEntropyLoss()

    # å¼€å§‹è®­ç»ƒ
    print("ğŸ‹ï¸ æ­£åœ¨è®­ç»ƒæ¨¡å‹...")
    for epoch in range(EPOCHS):
        total_loss = 0
        model.train()
        for images, labels in tqdm(train_loader):
            images, labels = images.to(device), labels.to(device)
            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
        print(f"ğŸ“˜ Epoch {epoch+1}/{EPOCHS} - Loss: {total_loss:.4f}", flush=True)

    # è¯„ä¼°æ¨¡å‹
    correct = 0
    total = 0
    model.eval()
    with torch.no_grad():
        for images, labels in test_loader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            _, predicted = torch.max(outputs, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
    print(f"ğŸ¯ æµ‹è¯•å‡†ç¡®ç‡: {100 * correct / total:.2f}%", flush=True)

    # ä¿å­˜æ¨¡å‹
    torch.save(model.state_dict(), MODEL_PATH)
    print(f"âœ… æ¨¡å‹ä¿å­˜æˆåŠŸï¼š{MODEL_PATH}", flush=True)
